{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![avatar](figures/HOG_visual.png)\n",
    "\n",
    "# SI100B Assignment 3 (Fall, 2021): \n",
    "# Appearance-based Gaze Estimation\n",
    "*******\n",
    "## Bonus Section: HOG with numpy\n",
    "<br/>\n",
    "\n",
    "Author: `Yintao Xu` | Modified by: `Yucong Chen`, `Yiwen Wu` | Proofread by: `Ziqi Gao`\n",
    "\n",
    "Email: `{xuyt, chenyc, wuyw1, gaozq}@shanghaitech.edu.cn` | Update: `2021-11` \n",
    "<br/><br/>\n",
    "\n",
    "Make sure you have pass the section1&2 before starting this bonus part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: run this cell before runnng any cell to activate auto re-import\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# 'gazelib' is the toolkit provided by this assignment, at the same directory of this notebook\n",
    "from gazelib.utils import *\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>yaw</th>\n",
       "      <th>pitch</th>\n",
       "      <th>image_base64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>-0.202976</td>\n",
       "      <td>-0.300898</td>\n",
       "      <td>/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.153875</td>\n",
       "      <td>-0.216009</td>\n",
       "      <td>/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>-0.240222</td>\n",
       "      <td>-0.231348</td>\n",
       "      <td>/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>0.082681</td>\n",
       "      <td>-0.148303</td>\n",
       "      <td>/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>0.141780</td>\n",
       "      <td>-0.259967</td>\n",
       "      <td>/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14</td>\n",
       "      <td>0.089140</td>\n",
       "      <td>-0.223431</td>\n",
       "      <td>/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>-0.052793</td>\n",
       "      <td>-0.056385</td>\n",
       "      <td>/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13</td>\n",
       "      <td>-0.275569</td>\n",
       "      <td>-0.115087</td>\n",
       "      <td>/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>0.095293</td>\n",
       "      <td>-0.210037</td>\n",
       "      <td>/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.066253</td>\n",
       "      <td>-0.250547</td>\n",
       "      <td>/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id       yaw     pitch  \\\n",
       "0           5 -0.202976 -0.300898   \n",
       "1           5  0.153875 -0.216009   \n",
       "2           6 -0.240222 -0.231348   \n",
       "3           6  0.082681 -0.148303   \n",
       "4          11  0.141780 -0.259967   \n",
       "5          14  0.089140 -0.223431   \n",
       "6           6 -0.052793 -0.056385   \n",
       "7          13 -0.275569 -0.115087   \n",
       "8           5  0.095293 -0.210037   \n",
       "9           2 -0.066253 -0.250547   \n",
       "\n",
       "                                        image_base64  \n",
       "0  /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...  \n",
       "1  /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...  \n",
       "2  /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...  \n",
       "3  /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...  \n",
       "4  /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...  \n",
       "5  /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...  \n",
       "6  /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...  \n",
       "7  /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...  \n",
       "8  /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...  \n",
       "9  /9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBw...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the training dataset\n",
    "train_df = load_train_csv_as_df()\n",
    "# **DO NOT MAKE ANY CHANGE TO IT THROUGHOUT THE NOTEBOOK**\n",
    "\n",
    "# previwe the first 10 samples\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to generate sampled dataset, it may take ~10s.\n",
      "train_X.shape: (4000, 18, 30)\n",
      "train_Y.shape: (4000, 2)\n",
      "val_X.shape: (1000, 18, 30)\n",
      "val_X.shape: (1000, 2)\n"
     ]
    }
   ],
   "source": [
    "# transform data into numpy arrays\n",
    "def df2nptensor(df):\n",
    "    imgs = []\n",
    "    imgs_HOG = []\n",
    "    gaze_dirs = []\n",
    "\n",
    "    print_interval = 1000\n",
    "    print_cnter = 0\n",
    "    \n",
    "    for _, i in df.iterrows():\n",
    "        if print_cnter % print_interval == 0:\n",
    "            print(\"[{} / {}]\".format(print_cnter, len(df)), end='\\r')\n",
    "        print_cnter += 1\n",
    "        im_arr = decode_base64_img(i['image_base64'])\n",
    "        gaze_dirs.append([i['yaw'], i['pitch']])\n",
    "        im = im_arr / 255\n",
    "        \n",
    "        imgs.append(im)\n",
    "    \n",
    "    gaze_dirs = np.array(gaze_dirs)\n",
    "    imgs = np.array(imgs)\n",
    "    \n",
    "    return gaze_dirs, imgs\n",
    "\n",
    "# For effciency, we only takes first 5,000 samples. Pick subject 5 as validation \n",
    "# set and the rest of the dataset as training set\n",
    "SAMPLE_NUM = 5000\n",
    "print(\"Start to generate sampled dataset, it may take ~10s.\")\n",
    "train_Y, train_X = df2nptensor(train_df[train_df[\"subject_id\"] != 5][: int(SAMPLE_NUM * 0.8)])\n",
    "val_Y, val_X = df2nptensor(train_df[train_df[\"subject_id\"] == 5][: int(SAMPLE_NUM * 0.2)])\n",
    "\n",
    "print(\"train_X.shape: {}\".format(train_X.shape))\n",
    "print(\"train_Y.shape: {}\".format(train_Y.shape))\n",
    "print(\"val_X.shape: {}\".format(val_X.shape))\n",
    "print(\"val_X.shape: {}\".format(val_Y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roadmap\n",
    "\n",
    "At previous sections, an estimation pipeline for gaze vector has been built:\n",
    "\n",
    "<img src=\"figures/gaze_model_pipeline.png\" style=\"zoom:80%\" />\n",
    "\n",
    "Formalize the input/ouput of our estimation pipeline mathematically:\n",
    "- `Input`: the gray-scale image $I\\in \\mathbb{R}^{18\\times30}$ (18x30 numpy array)\n",
    "- `Output`: two floats: yaw($\\gamma \\in \\mathbb{R}$), pitch($\\theta  \\in \\mathbb{R}$)\n",
    "\n",
    "\n",
    "In previous sections, we evaluated the simlarity between image by euclidean distance. However, naive computation over the whole images is computationally expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus-1: 2D Convolution[1] (10 points)\n",
    "Kernel is an important operator in image processing, which is used for blurring, sharpening, edge detection, etc. This is accomplished by doing a convolution between the kernel and an image. \n",
    "\n",
    "Convolution is the process of transforming an image by applying a kernel over each pixel and its local neighbors across the entire image. The kernel is a matrix of values whose size and values determine the transformation effect of the convolution process. \n",
    "\n",
    "\n",
    "The demo below shows how a typical 2D convolution works: you start with a kernel $\\textbf{F}$, the 3x3 moving square on the left, while the smaller numbers on the right-down corners are its weights. This kernel slides over the 2D input $\\textbf{I}$, the blue 5x5 square, performs an element-wise multiplication with the corresponding pixel it is over, and then sums the resultingmultiplied values and returns the resulting value as the new value of the center pixel before sliding to the next position. The green 3x3 square is the output $\\textbf{O}$ for this operation. \n",
    "\n",
    "![ChessUrl](figures/conv2d.gif) \n",
    "\n",
    "Formally, it is denoted as $*$:\n",
    "\n",
    "$$\n",
    "    \\textbf{O} = \\textbf{F} * \\textbf{I}\n",
    "$$\n",
    "\n",
    "with $\\textbf{O}$ corresponding to output, $\\textbf{F}\\in \\mathbb{R}^{2K+1\\times2K+1}$ corresponding to filter kernel, $\\textbf{I}\\in \\mathbb{R}^{W \\times H}$ corresponding to the input Image.\n",
    "\n",
    "The formula can be written as follows:\n",
    "\n",
    "$$\n",
    "    \\textbf{O}_{i,j} = \\sum_{k=0}^{2K} \\sum_{l=0}^{2K} \\textbf{F}_{k,l} \\times \\textbf{I}_{k + i, l + j}, i \\in [0, W -2], j \\in [0, H - 2]\n",
    "$$\n",
    "\n",
    "For instance, for the upper-left corner of output value 12 at previous figure:\n",
    "\n",
    "$$\n",
    "    12 = 3 \\times 0 + 3 \\times 1 + 2 \\times 2 + 0 \\times 2 + 0 \\times 2 + 1 \\times 0 + 3 \\times 0 + 1 \\times 1 + 2 \\times 2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figures/code_time.png)\n",
    "\n",
    "**Complete code at `AppearanceGazeEst/conv2d3x3`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "[[12. 12.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "[[12. 12. 17.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "[[12. 12. 17.]\n",
      " [10.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "[[12. 12. 17.]\n",
      " [10. 17.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "[[12. 12. 17.]\n",
      " [10. 17. 19.]\n",
      " [ 0.  0.  0.]]\n",
      "[[12. 12. 17.]\n",
      " [10. 17. 19.]\n",
      " [ 9.  0.  0.]]\n",
      "[[12. 12. 17.]\n",
      " [10. 17. 19.]\n",
      " [ 9.  6.  0.]]\n",
      "[[12. 12. 17.]\n",
      " [10. 17. 19.]\n",
      " [ 9.  6. 14.]]\n",
      "Pass Bonus-1 - convolution 2d local test.\n"
     ]
    }
   ],
   "source": [
    "# Local Test - Bonus-1\n",
    "# Note: feel free to print out your result to debug if it cannot pass assert_eq_np\n",
    "from AppearanceGazeEst import conv2d3x3\n",
    "from gazelib.task_2_judge import assert_eq_np\n",
    "\n",
    "img = np.array([\n",
    "    [3, 3, 2, 1, 0],\n",
    "    [0, 0, 1, 3, 1],\n",
    "    [3, 1, 2, 2, 3],\n",
    "    [2, 0, 0, 2, 2],\n",
    "    [2, 0, 0, 0, 1]\n",
    "])\n",
    "kernel = np.array([\n",
    "    [0, 1, 2],\n",
    "    [2, 2, 0],\n",
    "    [0, 1, 2]\n",
    "])\n",
    "gt_out = np.array([\n",
    "    [12, 12, 17],\n",
    "    [10, 17, 19],\n",
    "    [9, 6, 14]\n",
    "])\n",
    "\n",
    "out = conv2d3x3(img, kernel)\n",
    "assert_eq_np(gt_out, out)\n",
    "\n",
    "print(\"Pass Bonus-1 - convolution 2d local test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient of An Image [2]\n",
    "\n",
    "Gradient of an image is a directional change in the intensity or color in the image. \n",
    "\n",
    "There is a widely accepted assumption in computer vision community: \n",
    "> local object appearance and shape can often be characterized rather well by **the distribution of local intensity gradients or edge directions**, even without precise knowledge of the corresponding gradient or edge positions.\n",
    "\n",
    "Therefore, studying the **image gradient**[3] is important. \n",
    "\n",
    "![](figures/HOG.png)\n",
    "\n",
    "Mathematically, the gradient of a two-variable function (here the image intensity function) at each image point is a 2D vector with the components given by the derivatives in the horizontal and vertical directions.\n",
    "\n",
    "An image in modern devices can be viewed as a matrix of pixel intensity whose distribution is discrete. However, derivatives of this function cannot be defined unless we assume that there is an underlying continuous intensity function which has been sampled at the image points. \n",
    "\n",
    "The most common way to approximate the image gradient is to **convolve an image with a kernel**, such as the [**Sobel operator**](https://en.wikipedia.org/wiki/Sobel_operator), which is built on the **2D convolution** you've done above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus-2: Compute Gradient By Sobel Operator - Apply convolution by numpy [4] (10 points)\n",
    "The Sobel operator is a discrete differentiation operator, which can be used to compute an approximation of the gradient of the image intensity function.\n",
    "\n",
    "The operator uses two 3×3 kernels which are convolved with the original image to calculate approximations of the derivatives – one for horizontal changes, and one for vertical. The 2D convolution has been implemented by the previous section. If we define $\\textbf{I}$ as the source image, and $\\textbf{Gx}$ and $\\textbf{Gy}$ are two images, each of whose point contains the vertical and horizontal derivative approximations respectively. The computations are as follows:\n",
    "\n",
    "$$\n",
    "\\mathbf {G} _{x}={\\begin{bmatrix}-1&0&1\\\\-2&0&2\\\\-1&0&1\\end{bmatrix}}*\\mathbf {I} \\quad {\\mbox{and}}\\quad \\mathbf {G} _{y}={\\begin{bmatrix}-1&-2&-1\\\\0&0&0\\\\1&2&1\\end{bmatrix}}*\\mathbf {I}\n",
    "$$\n",
    "\n",
    "In most implementations, they also apply a gaussian kernel to smooth the image before getting gradients.\n",
    "\n",
    "![](figures/eye_grad_new.png)\n",
    "\n",
    "Formally, for each pixel\n",
    "$$\n",
    "Mag = \\sqrt{g _{x}^2 + g_{y}^2}\n",
    "$$\n",
    "$$\n",
    "\\mathbf{Dir} = [g _{x}; g_{y}]\n",
    "$$\n",
    "$$\n",
    "\\mathbf{Dir}_{norm} = \\mathbf{Dir} / (Mag + 10^{-3})\n",
    "$$\n",
    "Here, $10^{-3}$ is applied to improve numerical robustness in case of zero magnitude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figures/code_time.png)\n",
    "\n",
    "**Complete the code of the sobel gradient computation at `AppearanceGazeEst.py/compute_grad`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grad_dir_norm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-81-8c5531742e9f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m ])\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mgrad_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_mag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msanity_im\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0massert_eq_2223\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_mag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"You pass the local test - Bonus-2\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\眼动仪2\\AppearanceGazeEst.py\u001b[0m in \u001b[0;36mcompute_grad\u001b[1;34m(im)\u001b[0m\n\u001b[0;32m    393\u001b[0m     \u001b[1;31m############################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 395\u001b[1;33m     \u001b[1;32massert\u001b[0m \u001b[0mgrad_dir_norm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mim_blur\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mim_blur\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    396\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mgrad_mag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mim_blur\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'grad_dir_norm' is not defined"
     ]
    }
   ],
   "source": [
    "# Local Test - Bonus-2\n",
    "# Note: feel free to print out your result to debug at gazelib.task_2_judge.py/assert_eq_2223\n",
    "# if it cannot pass assert_eq_np\n",
    "from AppearanceGazeEst import compute_grad\n",
    "from gazelib.task_2_judge import assert_eq_2223\n",
    "\n",
    "sanity_im = np.array([\n",
    "    [1., 1., 7.], \n",
    "    [2., 1., 8.], \n",
    "    [3., 5., 2.]\n",
    "])\n",
    "\n",
    "grad_dir, grad_mag = compute_grad(sanity_im)\n",
    "assert_eq_2223(grad_dir, grad_mag)\n",
    "print(\"You pass the local test - Bonus-2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Now, playground!*\n",
    "\n",
    "- Run the following cell repeatedly and see what you get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample an image to visualize the gradient computation\n",
    "sample_im = train_X[random.randint(0, train_X.shape[0])]\n",
    "gaze_dir, gaze_mag = compute_grad(sample_im)\n",
    "vis.vis_grad(sample_im, gaze_dir[0] * gaze_mag, gaze_dir[1] * gaze_mag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of oriented gradient (HOG)\n",
    "Recall that\n",
    "> local object appearance and shape can often be characterized rather well by **the distribution of local intensity gradients or edge directions**, even without precise knowledge of the corresponding gradient or edge positions.\n",
    "\n",
    "When finishing computing gradients, in the next step, you should get its distribution over an image (or an image patch). Using histogram is an intuitive way to model a distribution with many weighted data points(gradients of pixels, with direction and magnitude). This step is called **Orientation Binning**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation: Why HOG? \n",
    "\n",
    "\n",
    "We have 42,000 images in our pandas data frame. Assume that in each estimation, we should do one floating number multplication for each pixels in every image.\n",
    "<br/><br/>\n",
    "\n",
    "#### *Now, playground!*\n",
    "- Run the following cell repeatedly and see what you get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randn(42000 * 18 * 30,)\n",
    "b = np.random.randn(42000 * 18 * 30,)\n",
    "\n",
    "time_start = time.time()\n",
    "c = a * b\n",
    "time_end = time.time()\n",
    "\n",
    "print(\"Multiply 42000 * 18 * 30 numbers cost: {:.4f}s\".format(time_end - time_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Large computational cost**\n",
    "\n",
    "It runs ~0.06 seconds on *Intel(R) Core(TM) i7-6700 CPU @ 3.40GHz*, which sounds tractable. However, you may repeat 10,000 ~ 20,000 such operation, which means roughly that it will cost ~ 20 min. Besides, memory cost is also considerable. Therefore, we require a **dimension reduction** method to reduce the computation. \n",
    "<br/><br/>\n",
    "**Not robust to translation**\n",
    "\n",
    "If we apply euclidean distance to images, when a slight translation of image occurs, it may lead to dramatic change to output.\n",
    "\n",
    "In computer vision community, **HOG** is a frequently used method to do the feature engineering of images with tractable size of dimension and more robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Details of Orientation Binning\n",
    "\n",
    "**Note**: You are not required to go to very details of Orientation Binning. We'll provide you with a reference code.\n",
    "\n",
    "<img src=\"figures/hist_grad_vis.png_fix.png\" style=\"zoom:60%\" />\n",
    "\n",
    "**Range of angle**[6] \n",
    "\n",
    "In this assignment, angles are derived from `np.arctan2` towards gradient direction(x1,x2), which means that it ranges from $[-\\pi, +\\pi]$.\n",
    "\n",
    "![](figures/arctan2_wiki.png)\n",
    "\n",
    "**Bilinear voting policy**[5] \n",
    "\n",
    "A bin is selected based on the direction, and the vote (the value that goes into the bin) is selected based on the magnitude. \n",
    "\n",
    "Let’s first focus on the pixel encircled in blue. It has an angle (direction) of 30 degrees and magnitude of 5. So it adds 5 to the 30-deg. bin. The gradient at the pixel encircled using red has an angle of -40 degrees and magnitude of 3. Since -40 lies in $[-60,-30]$, the vote by the pixel splits into the two bins proportional to 1/dist. to the bin, i.e.1 for -60 and 2 for -30. \n",
    "\n",
    "**Boundary condtion** \n",
    "\n",
    "When an angle goes to boundary, for example, 160 degree, it should be goes to bin of 150-deg and -180-deg. However, due to the range of `np.arctan2`, angle skips numerically at 180 and -180 but adajacent at cartesian coordination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus-3 Re-implement HOG - learn vectorization at numpy (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy: vectorization [7]\n",
    "\n",
    "Vectorization is used to speed up the Python code without using loop. Numpy will invokes C-code directly at backend, which is more rapid than python loops.\n",
    "<br/><br/>\n",
    "\n",
    "#### *Now, playground!*\n",
    "- Run the following cell repeatedly and see what you get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = int(1e7)\n",
    "# Try this cell to see the power of vectorization\n",
    "a = np.random.randn(size,)\n",
    "b = np.random.randn(size,)\n",
    "\n",
    "c = 0\n",
    "# python loops\n",
    "time_start = time.time()\n",
    "for i in range(size):\n",
    "    c = a[i] + b[i]\n",
    "time_nonvec = time.time() - time_start\n",
    "\n",
    "# vectorization\n",
    "time_start = time.time()\n",
    "c_ = np.sum(a + b)\n",
    "time_vec = time.time() - time_start\n",
    "\n",
    "speed_up_per = (time_nonvec / time_vec - 1) * 100\n",
    "\n",
    "print(\"Before vectorization: {:.3f} s\".format(time_nonvec))\n",
    "print(\"After vectorization: {:.3f} s\".format(time_vec))\n",
    "print(\"Speedup: {:.2f}%\".format(speed_up_per))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figures/code_time.png)\n",
    "\n",
    "**Complete the code of the HOG computation at `AppearanceGazeEst.py/bilinear_HOG`**\n",
    "\n",
    "**Note**: the non-vectorized version is at `ApperanceGazeEstV2/bilinear_HOG_nonvec` above the target function `bilinear_HOG`. Please gaurantee that your implementation shares the same output as the non-vectorized version and speeds up at least 200%. On the autolab, you'll get a full grade if it speeds up over 100%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local Test - Bonus-3\n",
    "# Note: feel free to print out your result to debug at gazelib.task_2_judge.py/assert_eq_2223\n",
    "# if it cannot pass assert_eq_np\n",
    "from gazelib.HOG import bilinear_HOG_patch_nonvec\n",
    "from AppearanceGazeEst import bilinear_HOG\n",
    "import time\n",
    "\n",
    "sample_im = train_X[5]\n",
    "grad_dir, grad_mag = compute_grad(sample_im)\n",
    "time_nonvec = 0\n",
    "time_vec = 0\n",
    "exp_num = 12\n",
    "\n",
    "for _ in range(exp_num): # repeat experiments due to randomness of OS scheduling\n",
    "    time_start = time.time()\n",
    "    a = bilinear_HOG_patch_nonvec(gaze_dir, gaze_mag)\n",
    "    time_nonvec += time.time() - time_start\n",
    "\n",
    "    time_start = time.time()\n",
    "    b = bilinear_HOG(gaze_dir, gaze_mag)\n",
    "    time_vec += time.time() - time_start\n",
    "\n",
    "speed_up_per = (time_nonvec / time_vec - 1) * 100\n",
    "assert_eq_np(a, b)\n",
    "assert speed_up_per > 200\n",
    "\n",
    "print(\"Before vectorization(avg): {:.4f} s\".format(time_nonvec / exp_num))\n",
    "print(\"After vectorization(avg): {:.4f} s\".format(time_vec / exp_num))\n",
    "print(\"Speedup: {:.2f}%\".format(speed_up_per))\n",
    "print(\"You pass the local test - Bonus-3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Now, playground!*\n",
    "\n",
    "- Run the following cell repeatedly and see what you get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize a random image\n",
    "random_idx = random.randint(0, train_X.shape[0])\n",
    "sample_im = train_X[random_idx]\n",
    "grad_dir, grad_mag = compute_grad(sample_im)\n",
    "\n",
    "hist = bilinear_HOG(grad_dir, grad_mag)\n",
    "\n",
    "vis.vis_HOG(sample_im, grad_dir[0], grad_dir[1], hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptor blocks\n",
    "To keep spatial information, in practice, HOG splits image into blocks to get histograms, then concatenate histograms in a fixed order(e.g: row major) to a vector.\n",
    "\n",
    "<img src=\"figures/desc_block.png\" style=\"zoom:60%\" />\n",
    "\n",
    "**Note**: In this section, we give the implementation of descriptor blocks based on your previous function of HOG. If you do not complete the bonus-3, maually modify `bilinear_HOG_DB` function to use non-vectorization version of HOG.\n",
    "<br/><br/>\n",
    "\n",
    "#### *Now, playground!*\n",
    "\n",
    "- Run the following cell repeatedly and see what you get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AppearanceGazeEst import bilinear_HOG_DB\n",
    "\n",
    "random_idx = random.randint(0, train_X.shape[0])\n",
    "sample_im = train_X[random_idx]\n",
    "grad_dir, grad_mag = compute_grad(sample_im)\n",
    "\n",
    "# You can modify hyperparameter patch_num here, though not recommended\n",
    "patch_num = (3, 4)\n",
    "hist = bilinear_HOG_DB(sample_im, patch_num=patch_num)\n",
    "\n",
    "vis.vis_HOG_full(sample_im, grad_dir[0], grad_dir[1], hist, patch_num=patch_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus-4 Combine HOG with K-NN (5 points)\n",
    "\n",
    "At the previous sections, we applied HOG to a raw image to transform it into a vector. Let's apply HOG (completed in the previous notebook) to all images here.\n",
    "\n",
    "Then, we could do KNN over the \"HOGed\" images. You would see the improvement of performance with respect to the previous section. Since our python-based implementation of HOG is inefficient, the estimation process may be a little bit long. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figures/code_time.png)\n",
    "\n",
    "**Complete the code of the HOG computation at `ApperanceGazeEstV2/KNN_HOG`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local Test - Eye images\n",
    "# Note: feel free to print out your result to debug if it cannot pass assert_eq_np\n",
    "from AppearanceGazeEst import KNN_HOG\n",
    "from gazelib.task_2_judge import assert_eq_np\n",
    "from gazelib.task_3_judge import compute_angle_error\n",
    "\n",
    "idx = 10\n",
    "# truncate the traininig set to improve the performance\n",
    "ret = KNN_HOG(train_X, train_Y, val_X[idx], 5)\n",
    "print(ret)\n",
    "assert_eq_np(ret, np.array([0.1283756,  -0.06597272]))\n",
    "\n",
    "print(\"Pass local test@Bonus-4 - eye images\")\n",
    "plt.figure(figsize=(4, 3))\n",
    "vis.visualize_est(val_X[idx], ret, val_Y[idx])\n",
    "plt.title(\"Angle Error: {:.3f}\".format(compute_angle_error(val_Y[idx], ret)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figures/good_job_banner.png)\n",
    "You should have completed **all cells(35 points)** in this task locally when you reach here!\n",
    "\n",
    "**CheckList**\n",
    "\n",
    "- conv2d (10 points)\n",
    "- compute_grad (10 points)\n",
    "- bilinear_HOG (10 points)\n",
    "- KNN_HOG (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "- [1] Intuitively Understanding Convolutions for Deep Learning: https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1\n",
    "- [2] Image gradient wiki: https://en.wikipedia.org/wiki/Image_gradient\n",
    "- [3] One of image gradient visualization is imported from: https://www.learnopencv.com/histogram-of-oriented-gradients/\n",
    "- [4] https://en.wikipedia.org/wiki/Sobel_operator\n",
    "- [5] https://www.learnopencv.com/histogram-of-oriented-gradients/\n",
    "- [6] https://en.wikipedia.org/wiki/Atan2\n",
    "- [7] https://www.geeksforgeeks.org/vectorization-in-python/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
